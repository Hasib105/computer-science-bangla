# GPU à¦†à¦°à§à¦•à¦¿à¦Ÿà§‡à¦•à¦šà¦¾à¦° à¦¬à¦¿à¦¸à§à¦¤à¦¾à¦°à¦¿à¦¤
## Chapter 17: GPU Architecture in Detail

---

## ğŸ“Œ NVIDIA GPU Architecture Evolution

```
Architecture Timeline:

Tesla (2006)     â†’ CUDA à¦¶à§à¦°à§
Fermi (2010)     â†’ Unified shader, ECC
Kepler (2012)    â†’ Dynamic Parallelism, Hyper-Q
Maxwell (2014)   â†’ Energy efficiency
Pascal (2016)    â†’ HBM2, NVLink
Volta (2017)     â†’ Tensor Cores
Turing (2018)    â†’ RT Cores, DLSS
Ampere (2020)    â†’ 2nd gen RT, 3rd gen Tensor
Ada Lovelace (2022) â†’ 3rd gen RT, 4th gen Tensor
Hopper (2022)    â†’ Transformer Engine
Blackwell (2024) â†’ Next-gen AI
```

---

## ğŸ—ï¸ Complete GPU Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        GPU Architecture                              â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    GPC (Graphics Processing Cluster)           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚                    TPC (Texture Processing Cluster)       â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                    SM (Streaming Multiprocessor)     â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                                                      â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚           Warp Schedulers (4)                 â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                                                      â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  CUDA Cores: 128 FP32 + 64 FP64              â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Tensor Cores: 4                              â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  RT Core: 1                                   â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Load/Store Units: 32                         â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Special Function Units (SFU): 16            â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚                                                      â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Register File: 256 KB                       â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â”‚  Shared Memory/L1 Cache: 128 KB              â”‚   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚ â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         L2 Cache (48 MB)                       â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚               Memory Controllers (384-bit GDDR6X)              â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§µ Warp Execution Model

### Warp à¦•à§€?
32 threads à¦à¦•à¦¸à¦¾à¦¥à§‡ SIMT (Single Instruction, Multiple Thread) fashion à¦ executeà¥¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Warp Execution                         â”‚
â”‚                                                              â”‚
â”‚  Warp 0: â”Œâ”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”¬â”€â”                 â”‚
â”‚          â”‚0â”‚1â”‚2â”‚3â”‚4â”‚5â”‚6â”‚7â”‚8â”‚9â”‚...       31â”‚                 â”‚
â”‚          â””â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”´â”€â”˜                 â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚              Same instruction executed                       â”‚
â”‚              on different data                               â”‚
â”‚                                                              â”‚
â”‚  Cycle 1: ADD R1, R2, R3  (all 32 threads)                  â”‚
â”‚  Cycle 2: MUL R4, R1, R5  (all 32 threads)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Warp Divergence

```
if (threadIdx.x < 16) {
    // Path A - threads 0-15
} else {
    // Path B - threads 16-31
}

Execution:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Cycle 1: Path A (threads 0-15 active, 16-31 idle)      â”‚
â”‚  Cycle 2: Path B (threads 0-15 idle, 16-31 active)      â”‚
â”‚                                                          â”‚
â”‚  Performance loss due to serialization!                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ CUDA Core Architecture

### FP32 CUDA Core
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CUDA Core (FP32)                         â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  Operand A  â”‚â”€â”€â”€â–¶â”‚             â”‚    â”‚             â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚    FMA      â”‚â”€â”€â”€â–¶â”‚  Result     â”‚     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  (Fused     â”‚    â”‚             â”‚     â”‚
â”‚  â”‚  Operand B  â”‚â”€â”€â”€â–¶â”‚  Multiply   â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚   Add)      â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚             â”‚                         â”‚
â”‚  â”‚  Operand C  â”‚â”€â”€â”€â–¶â”‚             â”‚                         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚                                                              â”‚
â”‚  Result = A Ã— B + C  (one cycle)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Special Function Units (SFU)
```
Transcendental functions:
- sin, cos, tan
- exp, log
- sqrt, rsqrt

~8 cycles per operation
```

---

## ğŸ§® Tensor Core Architecture

### Matrix Operations
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Tensor Core                             â”‚
â”‚                                                              â”‚
â”‚  4Ã—4 Matrix Multiply-Accumulate per cycle                   â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚  A      â”‚ Ã— â”‚  B      â”‚ + â”‚  C      â”‚ = â”‚  D      â”‚     â”‚
â”‚  â”‚ (4Ã—4)   â”‚   â”‚ (4Ã—4)   â”‚   â”‚ (4Ã—4)   â”‚   â”‚ (4Ã—4)   â”‚     â”‚
â”‚  â”‚ FP16    â”‚   â”‚ FP16    â”‚   â”‚ FP32    â”‚   â”‚ FP32    â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                              â”‚
â”‚  D = A Ã— B + C                                              â”‚
â”‚                                                              â”‚
â”‚  Performance: 256 FP16 operations per cycle per Tensor Core â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tensor Core Generations

| Generation | Matrix Size | Data Types |
|------------|-------------|------------|
| 1st (Volta) | 4Ã—4 | FP16 |
| 2nd (Turing) | 4Ã—4 | FP16, INT8, INT4 |
| 3rd (Ampere) | 8Ã—4 | FP16, BF16, TF32, INT8 |
| 4th (Ada) | 8Ã—4 | FP8, FP16, BF16, TF32 |

---

## ğŸ¯ RT Core (Ray Tracing)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RT Core                                â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              BVH Traversal Unit                      â”‚    â”‚
â”‚  â”‚  (Bounding Volume Hierarchy)                         â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Ray â”€â”€â”€â”€â–¶ BVH Tree â”€â”€â”€â”€â–¶ Intersection Test         â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚              Triangle Intersection Unit              â”‚    â”‚
â”‚  â”‚                                                      â”‚    â”‚
â”‚  â”‚  Ray + Triangle â”€â”€â”€â”€â–¶ Hit/Miss + Barycentric coords â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                                              â”‚
â”‚  Acceleration: ~10x vs software ray tracing                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¾ Memory System Details

### Global Memory (GDDR6X/HBM)

```
GDDR6X:
- 384-bit bus width
- 21 Gbps per pin
- 1 TB/s bandwidth

HBM3 (Data Center):
- 6144-bit bus width
- 3.35 TB/s bandwidth
- Stacked memory

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Memory Coalescing                        â”‚
â”‚                                                              â”‚
â”‚  Efficient (Coalesced):                                      â”‚
â”‚  Thread 0 â”€â–¶ addr[0]                                        â”‚
â”‚  Thread 1 â”€â–¶ addr[1]   â•â•â–¶ Single memory transaction        â”‚
â”‚  Thread 2 â”€â–¶ addr[2]                                        â”‚
â”‚  ...                                                         â”‚
â”‚                                                              â”‚
â”‚  Inefficient (Non-coalesced):                               â”‚
â”‚  Thread 0 â”€â–¶ addr[0]                                        â”‚
â”‚  Thread 1 â”€â–¶ addr[100]  â•â•â–¶ Multiple memory transactions    â”‚
â”‚  Thread 2 â”€â–¶ addr[200]                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Shared Memory

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Shared Memory Banks                       â”‚
â”‚                                                              â”‚
â”‚  32 banks, 4 bytes per bank                                 â”‚
â”‚                                                              â”‚
â”‚  Bank 0 â”‚ Bank 1 â”‚ Bank 2 â”‚ ... â”‚ Bank 31                   â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”‚
â”‚  [0]    â”‚ [1]    â”‚ [2]    â”‚     â”‚ [31]                      â”‚
â”‚  [32]   â”‚ [33]   â”‚ [34]   â”‚     â”‚ [63]                      â”‚
â”‚                                                              â”‚
â”‚  Bank Conflict: Multiple threads access same bank           â”‚
â”‚  â†’ Serialization                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Thread Scheduling

### Warp Scheduler

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Warp Scheduling                           â”‚
â”‚                                                              â”‚
â”‚  Ready Queue:                                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚ Warp 0 â”‚ Warp 2 â”‚ Warp 5 â”‚ Warp 7 â”‚ ...    â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚                                                              â”‚
â”‚  Stalled Queue (waiting for memory):                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚  â”‚ Warp 1 â”‚ Warp 3 â”‚ Warp 4 â”‚                              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                                                              â”‚
â”‚  Scheduler: Zero-overhead context switch                    â”‚
â”‚  - Warp stalls â†’ immediately switch to ready warp           â”‚
â”‚  - Latency hiding through parallelism                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Occupancy

```
Occupancy = Active Warps / Max Warps per SM

Factors affecting occupancy:
- Registers per thread
- Shared memory per block
- Block size

Higher occupancy â†’ Better latency hiding
```

---

## ğŸ”Œ Interconnects

### NVLink
```
GPU-to-GPU high-speed interconnect

NVLink 4.0:
- 900 GB/s bidirectional
- Up to 18 links per GPU

â”Œâ”€â”€â”€â”€â”€â” â—€â•â•â•â•â•â•â•â•â•â•â–¶ â”Œâ”€â”€â”€â”€â”€â”
â”‚GPU 0â”‚    NVLink     â”‚GPU 1â”‚
â””â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”˜
```

### PCIe
```
PCIe 5.0 x16:
- 128 GB/s bidirectional
- CPU-GPU communication
```

---

## âœï¸ à¦…à¦¨à§à¦¶à§€à¦²à¦¨à§€

1. Warp divergence à¦•à§€ à¦à¦¬à¦‚ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦à¦¡à¦¼à¦¾à¦¨à§‹ à¦¯à¦¾à¦¯à¦¼?
2. Tensor Core à¦à¦° à¦•à¦¾à¦œ à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à§à¦¨à¥¤
3. Memory coalescing à¦•à§‡à¦¨ à¦—à§à¦°à§à¦¤à§à¦¬à¦ªà§‚à¦°à§à¦£?

---

## ğŸ“š à¦ªà¦°à¦¬à¦°à§à¦¤à§€ à¦…à¦§à§à¦¯à¦¾à¦¯à¦¼
[CUDA à¦“ à¦ªà§à¦¯à¦¾à¦°à¦¾à¦²à§‡à¦² à¦ªà§à¦°à§‹à¦—à§à¦°à¦¾à¦®à¦¿à¦‚](./03-CUDA-à¦ªà§à¦¯à¦¾à¦°à¦¾à¦²à§‡à¦²-à¦ªà§à¦°à§‹à¦—à§à¦°à¦¾à¦®à¦¿à¦‚.md)
