# à¦ªà§à¦¯à¦¾à¦°à¦¾à¦²à§‡à¦² à¦•à¦®à§à¦ªà¦¿à¦‰à¦Ÿà¦¿à¦‚
## Chapter 13: Parallel Computing

---

## ğŸ“Œ à¦ªà§à¦¯à¦¾à¦°à¦¾à¦²à§‡à¦² à¦•à¦®à§à¦ªà¦¿à¦‰à¦Ÿà¦¿à¦‚ à¦•à§€?

à¦ªà§à¦¯à¦¾à¦°à¦¾à¦²à§‡à¦² à¦•à¦®à§à¦ªà¦¿à¦‰à¦Ÿà¦¿à¦‚ à¦¹à¦²à§‹ à¦à¦•à¦¾à¦§à¦¿à¦• à¦ªà§à¦°à¦¸à§‡à¦¸à¦¿à¦‚ à¦‰à¦ªà¦¾à¦¦à¦¾à¦¨ à¦à¦•à¦¸à¦¾à¦¥à§‡ à¦¬à§à¦¯à¦¬à¦¹à¦¾à¦° à¦•à¦°à§‡ à¦¸à¦®à¦¸à§à¦¯à¦¾ à¦¸à¦®à¦¾à¦§à¦¾à¦¨ à¦•à¦°à¦¾à¦° à¦•à§Œà¦¶à¦²à¥¤

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Serial vs Parallel                              â”‚
â”‚                                                              â”‚
â”‚  Serial:                                                     â”‚
â”‚  Task â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â–¶ Done             â”‚
â”‚                                                              â”‚
â”‚  Parallel:                                                   â”‚
â”‚  Task1 â•â•â•â•â•â•â•â•â•â•â•â–¶                                         â”‚
â”‚  Task2 â•â•â•â•â•â•â•â•â•â•â•â–¶  â•â•â–¶ Combined Result â•â•â–¶ Done          â”‚
â”‚  Task3 â•â•â•â•â•â•â•â•â•â•â•â–¶                                         â”‚
â”‚  Task4 â•â•â•â•â•â•â•â•â•â•â•â–¶                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Flynn's Classification

### SISD (Single Instruction, Single Data)
```
Traditional uniprocessor

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CU     â”‚â”€â”€â”€â–¶â”‚   PU     â”‚â”€â”€â”€â–¶â”‚  Memory  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### SIMD (Single Instruction, Multiple Data)
```
à¦à¦•à¦‡ instruction, à¦…à¦¨à§‡à¦• data à¦

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CU     â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚
     â”œâ”€â”€â”€â”€â”€â–¶ PU1 â”€â”€â”€â–¶ Data1
     â”œâ”€â”€â”€â”€â”€â–¶ PU2 â”€â”€â”€â–¶ Data2
     â”œâ”€â”€â”€â”€â”€â–¶ PU3 â”€â”€â”€â–¶ Data3
     â””â”€â”€â”€â”€â”€â–¶ PU4 â”€â”€â”€â–¶ Data4

Example: GPU, Vector processors
```

### MISD (Multiple Instruction, Single Data)
```
à¦à¦•à¦‡ data à¦ à¦­à¦¿à¦¨à§à¦¨ instruction
à¦¬à¦¾à¦¸à§à¦¤à¦¬à§‡ à¦¬à¦¿à¦°à¦²à¥¤
```

### MIMD (Multiple Instruction, Multiple Data)
```
à¦­à¦¿à¦¨à§à¦¨ instruction, à¦­à¦¿à¦¨à§à¦¨ data

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CU1    â”‚â”€â”€â”€â–¶â”‚   PU1    â”‚â”€â”€â”€â–¶ Data1
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   CU2    â”‚â”€â”€â”€â–¶â”‚   PU2    â”‚â”€â”€â”€â–¶ Data2
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Example: Multicore processors, Clusters
```

---

## ğŸ—ï¸ Parallel Architecture Types

### Shared Memory
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Shared Memory                         â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚ P1  â”‚   â”‚ P2  â”‚   â”‚ P3  â”‚   â”‚ P4  â”‚                 â”‚
â”‚  â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜   â””â”€â”€â”¬â”€â”€â”˜                 â”‚
â”‚     â”‚         â”‚         â”‚         â”‚                     â”‚
â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                    â”‚                                     â”‚
â”‚              â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”                              â”‚
â”‚              â”‚  Memory   â”‚                              â”‚
â”‚              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

à¦¸à§à¦¬à¦¿à¦§à¦¾: Easy programming
à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾: Scalability limited
```

### Distributed Memory
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Distributed Memory                      â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ P1 â”‚ Mem1   â”‚   â”‚ P2 â”‚ Mem2   â”‚   â”‚ P3 â”‚ Mem3   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚         â”‚                 â”‚                 â”‚           â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                           â”‚                             â”‚
â”‚                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚                  â”‚   Interconnect   â”‚                   â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

à¦¸à§à¦¬à¦¿à¦§à¦¾: High scalability
à¦…à¦¸à§à¦¬à¦¿à¦§à¦¾: Complex programming (message passing)
```

---

## âš¡ Amdahl's Law

à¦¸à¦°à§à¦¬à§‹à¦šà§à¦š à¦•à¦¤à¦Ÿà§à¦•à§ speedup à¦¸à¦®à§à¦­à¦¬à¥¤

```
Speedup = 1 / ((1-P) + P/N)

P = Parallelizable fraction
N = Number of processors

à¦‰à¦¦à¦¾à¦¹à¦°à¦£:
P = 0.95 (95% parallelizable)
N = 16 processors

Speedup = 1 / ((1-0.95) + 0.95/16)
        = 1 / (0.05 + 0.059)
        = 1 / 0.109
        â‰ˆ 9.2x

à¦¸à¦°à§à¦¬à§‹à¦šà§à¦š (N = âˆ):
Speedup_max = 1 / (1-P) = 1/0.05 = 20x
```

```
Speedup vs Processors (P = 0.95):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 20 â”œâ”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ Maximumâ”‚
â”‚    â”‚                           âˆ™âˆ™âˆ™âˆ™âˆ™âˆ™âˆ™âˆ™âˆ™â”‚
â”‚ 15 â”‚                    âˆ™âˆ™âˆ™            â”‚
â”‚    â”‚               âˆ™âˆ™âˆ™                  â”‚
â”‚ 10 â”‚          âˆ™âˆ™                        â”‚
â”‚    â”‚       âˆ™                            â”‚
â”‚ 5  â”‚    âˆ™                               â”‚
â”‚    â”‚  âˆ™                                 â”‚
â”‚ 1  â”‚âˆ™                                   â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚      1   4   8  16  32  64  âˆ  Processors
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ˆ Gustafson's Law

Problem size à¦¬à¦¾à¦¡à¦¼à¦¾à¦²à§‡ better speedupà¥¤

```
Speedup = S + P Ã— N

S = Serial fraction
P = Parallel fraction
N = Number of processors
```

---

## ğŸ”„ Parallel Programming Models

### à§§. Shared Memory (Threads)

```c
// OpenMP Example
#pragma omp parallel for
for (int i = 0; i < n; i++) {
    a[i] = b[i] + c[i];
}
```

### à§¨. Message Passing (MPI)

```c
// MPI Example
MPI_Send(&data, count, MPI_INT, dest, tag, MPI_COMM_WORLD);
MPI_Recv(&data, count, MPI_INT, source, tag, MPI_COMM_WORLD, &status);
```

### à§©. Data Parallel (GPU)

```c
// CUDA Example
__global__ void add(int *a, int *b, int *c) {
    int i = threadIdx.x;
    c[i] = a[i] + b[i];
}
```

---

## ğŸ”’ Synchronization

### Race Condition
```
Thread 1: x = x + 1
Thread 2: x = x + 1

Expected: x = x + 2
Actual: Could be x + 1 (race!)
```

### Mutex (Mutual Exclusion)
```
Thread 1:              Thread 2:
lock(mutex)           
x = x + 1              wait...
unlock(mutex)          lock(mutex)
                       x = x + 1
                       unlock(mutex)
```

### Barrier
```
Thread 1    Thread 2    Thread 3
   â”‚           â”‚           â”‚
   â–¼           â–¼           â–¼
  Work        Work        Work
   â”‚           â”‚           â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
           BARRIER
               â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚           â”‚           â”‚
   â–¼           â–¼           â–¼
  Next        Next        Next
```

---

## ğŸ“Š Parallel Patterns

### à§§. Map
```
à¦ªà§à¦°à¦¤à¦¿à¦Ÿà¦¿ element à¦ à¦à¦•à¦‡ operation

Input:  [1, 2, 3, 4]
Map(xÂ²): [1, 4, 9, 16]
```

### à§¨. Reduce
```
à¦¸à¦¬ element à¦à¦•à¦¤à§à¦°à¦¿à¦¤

Input:  [1, 2, 3, 4]
Reduce(+): 10
```

### à§©. Scatter/Gather
```
Scatter: Data à¦¬à¦¿à¦¤à¦°à¦£
Gather: Result à¦¸à¦‚à¦—à§à¦°à¦¹
```

### à§ª. Pipeline
```
Stage 1 â”€â”€â–¶ Stage 2 â”€â”€â–¶ Stage 3 â”€â”€â–¶ Output
```

---

## âœï¸ à¦…à¦¨à§à¦¶à§€à¦²à¦¨à§€

1. Flynn's Classification à¦¬à§à¦¯à¦¾à¦–à§à¦¯à¦¾ à¦•à¦°à§à¦¨à¥¤
2. Amdahl's Law: 80% parallelizable, 8 processors à¦¹à¦²à§‡ speedup à¦•à¦¤?
3. Race condition à¦•à§€ à¦à¦¬à¦‚ à¦•à¦¿à¦­à¦¾à¦¬à§‡ à¦à¦¡à¦¼à¦¾à¦¨à§‹ à¦¯à¦¾à¦¯à¦¼?

---

## ğŸ“š à¦ªà¦°à¦¬à¦°à§à¦¤à§€ à¦…à¦§à§à¦¯à¦¾à¦¯à¦¼
[à¦®à¦¾à¦²à§à¦Ÿà¦¿à¦•à§‹à¦° à¦ªà§à¦°à¦¸à§‡à¦¸à¦°](./02-à¦®à¦¾à¦²à§à¦Ÿà¦¿à¦•à§‹à¦°-à¦ªà§à¦°à¦¸à§‡à¦¸à¦°.md)
